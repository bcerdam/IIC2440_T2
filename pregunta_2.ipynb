{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b136b71",
   "metadata": {},
   "source": [
    "# Pregunta 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6deb7e",
   "metadata": {},
   "source": [
    "Instalamos pyspark en caso de que sea necesario para correrlo en google colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "AJMoLDUrDLYS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AJMoLDUrDLYS",
    "outputId": "2c62d32e-d728-43cc-c285-5e17ab60227c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=6157aa670300c7f861a972d7d6a2ed748a374d9d910a5b640e13b068479411ad\n",
      "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.4.1\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49f0aba",
   "metadata": {},
   "source": [
    "Creamos contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69ce3c43",
   "metadata": {
    "id": "69ce3c43"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Contexto\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74c3012",
   "metadata": {},
   "source": [
    "Funcion que inicializa grafo como RDD en formato: (nodo_source, [nodo_target, peso, peso_acumulado])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "760de197",
   "metadata": {
    "id": "760de197"
   },
   "outputs": [],
   "source": [
    "# Inicializa pesos de grafo\n",
    "def inicializar(G, source):\n",
    "    G_pesos = [(node[0], [node[1][0], node[1][1], float('inf')]) if node[0] != source else (node[0], [node[1][0], node[1][1], 0]) for node in G]\n",
    "    return sc.parallelize(G_pesos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d77519",
   "metadata": {},
   "source": [
    "Inicializamos Grafo de prueba chico y lo restructuramos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92311fbe",
   "metadata": {
    "id": "92311fbe"
   },
   "outputs": [],
   "source": [
    "# Grafo de prueba\n",
    "G = [(1, [2, 1]), (1, [3, 4]), (2, [3, 2]), (2, [4, 3]), (3, [4, 1])]\n",
    "source = 1\n",
    "G_init = inicializar(G, source)\n",
    "\n",
    "# Re-estructura llave-valor (nodo, distancia)\n",
    "distancias = G_init.map(lambda x: (x[0], x[1][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8fbd849",
   "metadata": {
    "id": "a8fbd849"
   },
   "outputs": [],
   "source": [
    "# Si costo no cambia => break\n",
    "while True:\n",
    "    actualizaciones = G_init.join(distancias) \\\n",
    "        .flatMap(lambda x: [(x[1][0][0], x[1][0][1] + x[1][1]), (x[0], x[1][1])]) \\\n",
    "        .reduceByKey(min)\n",
    "\n",
    "    distancias_nuevas = distancias.fullOuterJoin(actualizaciones) \\\n",
    "        .mapValues(lambda x: min(x[0] or float('inf'), x[1] or float('inf')))\n",
    "\n",
    "    cambio_costo = distancias_nuevas.join(distancias) \\\n",
    "        .filter(lambda x: x[1][0] != x[1][1]) \\\n",
    "        .count()\n",
    "\n",
    "    distancias = distancias_nuevas\n",
    "\n",
    "    if cambio_costo == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5663fcd",
   "metadata": {},
   "source": [
    "Printeamos distancias entre nodo source y los caminos mas cortos hacia los otros nodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfe25b90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfe25b90",
    "outputId": "e9d9f2f5-364b-45fc-b9ec-7e6cb949eff8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de SSSP desde nodo 1 (Source) hacia nodo 2 es 1\n",
      "Valor de SSSP desde nodo 1 (Source) hacia nodo 3 es 3\n",
      "Valor de SSSP desde nodo 1 (Source) hacia nodo 4 es 4\n"
     ]
    }
   ],
   "source": [
    "nodos_printeados = set()\n",
    "for nodo, distancia in distancias.filter(lambda x: x[0] != source).collect():\n",
    "    if nodo not in nodos_printeados:\n",
    "        print(f\"Valor de SSSP desde nodo {source} (Source) hacia nodo {nodo} es {distancia}\")\n",
    "        nodos_printeados.add(nodo)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
